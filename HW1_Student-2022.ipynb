{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0abe9c-30b2-4513-8527-60b549430ecf",
   "metadata": {},
   "source": [
    "# Deep Learning in Medicine\n",
    "### BMSC-GA 4493, BMIN-GA 3007 \n",
    "### Spring 2022\n",
    "### Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4081173-2a9d-45e1-8779-aeb58b61ae91",
   "metadata": {},
   "source": [
    "**Learning Objectives**:\n",
    "\n",
    "1. Basic Math Revision.\n",
    "2. Introduction to Machine Learning.\n",
    "3. Logistic Regression Model.\n",
    "4. Multi-layer Perceptron Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a86b1-d8c7-444e-b0a4-3064ffd979cc",
   "metadata": {},
   "source": [
    "**Instruction** \n",
    "\n",
    "1. If you need to write mathematical terms, you can type your answeres in a Markdown Cell via LaTex. See: <a href=\"https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook\">here</a> if you have issues with writing equations. To see basic LaTex notation see: <a href=\"https://en.wikibooks.org/wiki/LaTeX/Mathematics\"> here </a>.\n",
    "\n",
    "2. Upload and Submit your final jupyter notebook file on <a href='https://brightspace.nyu.edu/d2l/home/158477'>Brightspace</a>\n",
    "\n",
    "3. **Deadline: Thursday Feb 17th 2021****\n",
    "\n",
    "4. ***HW submission instructions:*** Students should submit a zipped folder named ***netid***_hw***x*** where x is the homework number . The submission should consist of jupyter notebook with all the plots and expected outputs clearly visible in it. The zipped folder should also contain the data files. We should be able to run your ipynb without making directory changes. Not following the protocol might lead to deduction of scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203b91d-8e1d-430c-9690-eebd306420cb",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 1: Math and Machine Learning Revision (20 points)\n",
    "\n",
    "- ### Take derivatives (partial derivatives wherever required) of functions from 1.1 to 1.8 (16 points)\n",
    "- ### Derive appropriate loss for 1.9 (4 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2abe1c-29d7-420f-a8aa-29567327118e",
   "metadata": {},
   "source": [
    "### 1.1. (2 point)\n",
    "$f(x) = \\frac{1}{\\sqrt{3-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc181c9-a8c7-482f-acf7-01101f50d9b5",
   "metadata": {},
   "source": [
    "### 1.2. (2 point)\n",
    "$f(x) = e^xx^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1425b8e-8ac8-4a5e-8ee2-a525fe933f0e",
   "metadata": {},
   "source": [
    "### 1.3. (2 point)\n",
    "$f(x) = \\frac{e^x + \\sin x}{1+\\log x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a39fca-e55d-4ef1-9591-8a1b789db63c",
   "metadata": {},
   "source": [
    "### 1.4. (2 point)\n",
    "$f(x) = \\tanh x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719cc158-3adc-4424-b060-cef6ed03afd1",
   "metadata": {},
   "source": [
    "### 1.7. (4 point)\n",
    "$f(A,X) = \\sum_{i=1}^5 \\ln (\\frac{1}{a_ix_i^3})$\n",
    "\n",
    "where $A = (a_1, a_2, a_3, a_4, a_5)$ and $X = (x_1, x_2, x_3, x_4, x_5)$ (You can give expression of $\\frac{d}{dx_i} f(A,X)$ as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04376be-5f49-44a9-8e7d-cf31d3ef2a85",
   "metadata": {},
   "source": [
    "### 1.8. (4 point)\n",
    "$f(A,X) = \\sum_{i=1}^{100} (\\frac{1}{3x_i + e^{-b_ix_i}})$\n",
    "\n",
    "where $A = (a_1, ..., a_{100})$ and $X = (x_1, ..., x_{100})$ (You can give expression of $\\frac{d}{dx_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7937ec-d25a-46ab-ac82-37694dc1dc09",
   "metadata": {},
   "source": [
    "### 1.9.  Loss Functions (2 + 2 points)\n",
    "Mammography is one of the most effective method for breast cancer screening. However, the low positive predictive value of breast biopsy resulting from mammogram interpretation might lead to unnecessary biopsies with benign outcomes. To reduce the high number of unnecessary breast biopsies, you have been assigned to develop a machine learning model that will act as a computer-aided diagnosis (CAD) system. \n",
    "\n",
    "To help physicians in their decision to perform a breast biopsy on a suspicious lesion or to perform a short term follow-up examination, your model should identify the mammographic mass lesion as Benign(0)/Malign(1) given its BI-RADS attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae440289-5b29-40c4-af47-9ac396a0e180",
   "metadata": {},
   "source": [
    "#### 1.9.a What kind of supervised learning problem would you consider? Express your model's probability for one observation $p(y|x)$? Give expression of log probability as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a78623-e51a-48a3-aa46-624da1d66d25",
   "metadata": {},
   "source": [
    "#### 1.9.b You want to design a loss function that prefers the correct output of the training examples to be more likely. \n",
    "\n",
    "#### Derive a loss function that would allow you to choose parameters $w,b$ of your model that maximize a likelihood that does that. Give explaination would such a loss function be good choice in terms of parameter updation.\n",
    "\n",
    "*Hint:* Think about the expression from the previous part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa90fb5-bc6a-418e-9724-a779146a67ce",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 2: Solving Linear Regression via Mean Squared Error (MSE) Optimization Problem (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804e3e1-9d45-42c4-9b91-26b43dd7002a",
   "metadata": {},
   "source": [
    "Imagine that you have measured two variables X and Y, for a simple task, and you belive that they might be linearly related to each other. Here, our input X has 2 dimensions, and the output has 1 dimension. We will use super-script to indicate which sample it is, and sub-scipt to indicate which dimension it is. \n",
    "The measurements are as follows:\n",
    "\n",
    "###### (Training data D = {($X^1$, $Y^1$), ($X^2$, $Y^2$), ($X^3$, $Y^3$)})\n",
    "\n",
    "Sample 1: $X^1 = (x_1^1, x_2^1) = (1, 1)$,   $Y^1$ = 2\n",
    "\n",
    "Sample 2: $X^2 = (x_1^2, x_2^2) = (1, 2)$,   $Y^2$ = 4\n",
    "\n",
    "Sample 3: $X^3 = (x_1^3, x_2^3) = (2, 2)$,   $Y^3$ = 5\n",
    "\n",
    "If we assume that the relationship between X and Y is linear, we can write this relationship as:If we assume that the relationship between X and Y is linear, we can write this relationship as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a4a5c-5631-43c0-a3b2-4d2e1ffff3da",
   "metadata": {},
   "source": [
    "$Y = f_{W,B}(X) = WX + B = w_1*x_1 + w_2*x_2 + B$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812d9e5-070f-4208-a867-c7cce6a89e57",
   "metadata": {},
   "source": [
    "where $W = (w_1, w_2)$ and $B$ are the parameters of the model.\t\n",
    "We are interested in finding best values for W and B.\t\n",
    "We define 'best' in terms of a loss function between $f_{W,b}(X_i)$ and $Y_i$ for each ($X_i$ and $Y_i$) in the training data. \t\n",
    "Since $Y_i$s are real numbers, let's consider Mean Squared Error loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759cd032-8275-4db6-8f1b-45f9c913c956",
   "metadata": {},
   "source": [
    "Remember that Mean Squared Error for this function, over training data, and W and B is:\n",
    "\n",
    "$MSELoss(D={(X_1, Y_1), (X_2, Y_2), (X_3, Y_3)}), W, B) = \\frac{1}{3}\\sum_{i=1}^{3} (f_{W,B}(X_i) - Y_i)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc1d53-1dfc-4496-a793-df9793c77b58",
   "metadata": {},
   "source": [
    "### 2.1. (3 points)\n",
    "Compute the partial derivative of $MESLoss(D, W, B)$, With respect to W and B.\t\n",
    "Remember that $X_1$, $X_2$, $X_3$, $Y_1$, $Y_2$, and $Y_3$ are constants, and already given to us as training data above.\n",
    "\n",
    "$\\frac{d}{d w_1} MSELoss(D, W, B) = ?$\n",
    "\n",
    "$\\frac{d}{d w_2} MSELoss(D, W, B) = ?$\n",
    "\n",
    "$\\frac{d}{d B} MSELoss(D, W, B) = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66dbd0-4cb8-488e-ac61-86913b3c70db",
   "metadata": {},
   "source": [
    "### 2.2. (3 points)\n",
    "Use matplotlib library and plot $\\frac{d}{d w1} MSELoss(D, W, B)$ for $w_1 = np.arange(0, 5, 0.5)$, when $w_2$ equals 4, and B equals to -2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b68628-8d2c-4b70-a8b2-51b9cddd8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "w1 = np.arange(0, 5, 0.5)\n",
    "\n",
    "# plot dMSELoss/dw1 here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a817d3-ceff-4ddb-9098-22ca0360ad3d",
   "metadata": {},
   "source": [
    "### 2.3. (4 points)\n",
    "What values of $w_1$, $w_2$ and $B$, make all partial derivatives zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b299a1e-f912-4960-b349-ea51180543af",
   "metadata": {},
   "source": [
    "### 2.4) (10 points) \n",
    "If you start from an initial point $w_1^0 = 0.1$ , $w_2^0 = 0.1$ and $B^0 = 0.1$, and iteratively update your $w_1$, $w_2$, and B via gradient descent as follows:\n",
    "    \n",
    "$ w_1^{t+1} = w_1^t - 0.01 * \\frac{d}{d w_1} MSELoss(D, W, B) |_{w_1^t,w_2^t,B^t} $\t\n",
    "$ w_2^{t+1} = w_2^t - 0.01 * \\frac{d}{d w_2} MSELoss(D, W, B) |_{w_1^t,w_2^t,B^t} $\t\n",
    "$ B^{t+1} = B^t - 0.01 * \\frac{d}{d B} MSELoss(D, W, B) |_{w_1^t,w_2^t,B^t} $\n",
    "(Note: This is gradient descent with a 0.01 learning rate.)\n",
    "\n",
    "What are the values of Ws and B over iterations 0 to 2000? (Don't compute by hand! Write a code!)\n",
    "Write a python script that computes these values for 2000 iterations, i.e. lists of $\\{w_1^0, w^1_1,.., w_1^{2000}\\}$, $\\{w_2^0, w_2^1,.., w_2^{2000}\\}$, and $\\{B^0, B^1,.., B^{2000}\\}$.\t\n",
    "Plot the lists of $w_1$s, $w_2$s and Bs over 2000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd9631-9e48-46fa-a475-063b22d7ee98",
   "metadata": {},
   "source": [
    "### 2.5. (10 points)\n",
    "Now that you learned the math and made the code yourself, we will use pytorch and automatic differentiation, to find optimal W and B!\t\n",
    "Again, consider data to be D = {($X_1$, $Y_1$), ($X_2$, $Y_2$), ($X_3$, $Y_3$)}) = {((2,2), 5), ((2,7), 3), ((-1,0), 1)}.\n",
    "\n",
    "Some of your steps are here. Fill in the rest and show a plot of the loss function, $w_1$, $w_2$ and B over these 2000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b479f7-cba3-40c9-8786-ffb505b8a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "D = [((1,1), 2), ((1,2), 4), ((2,2),5)]\n",
    "X = [d[0] for d in D]\n",
    "Y = [d[1] for d in D]\n",
    "print('data X is:', X)\n",
    "print('data Y is:', Y)\n",
    "\n",
    "model = torch.nn.Linear(2, 1, bias=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "losslist = []\n",
    "w1list = []\n",
    "w2list = []\n",
    "blist = []\n",
    "\n",
    "\n",
    "# for epoch in range(10):\n",
    "    # Shuffle your training data samples\n",
    "    # Loop over your training data in the new order:\n",
    "        #dont forget to: optimizer.zero_grad()\n",
    "        #prepare your x_input and y_target if needed\n",
    "        #send the data through your model: i.e. pred_i = model(x_input)\n",
    "        #send the prediction through the loss function too: i.e. lossout= loss(pred_i, y_target)\n",
    "        #call backward to back-propagate: i.e. lossout.backward()\n",
    "        #call optimizer.step() to update the model parameters based on the computed gradients\n",
    "        #keep the w1s, w2s, and bs, and loss value some list so you can plot them later\n",
    "\n",
    "#plot the losslist, w1s, w2s, and bs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c27b9-8d47-47c6-9a38-491ca66949dd",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 3: Learning Curves, Overfitting, and Machine Learning (50 points + Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53b583-2fd0-40de-ab90-4b7bb3f30066",
   "metadata": {},
   "source": [
    "Now we know how to optimize, let's get some real machine learning done!\t\n",
    "\n",
    "Instead of the small dataset we had in questions 2, now let's use the the CBIS-DDSM (Curated Breast Imaging Subset of DDSM) dataset from <a href=\"https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM#385f2cd4e86f4142b1d32bdb5803bd96\"> here</a>\n",
    "\n",
    "\n",
    "In this homework, we will *only* focus on the following items in the dataset:\t\n",
    "**Mass-Training-Description (csv)**\t\n",
    "**Mass-Test-Description (csv)**\t\n",
    "<p><strong style=\"color: red;\">(Don't download the images data! They are not required for this homework)</strong></p>\n",
    "\n",
    "This dataset contains several features related to Mammography and detection of breast cancer. \n",
    "\n",
    "The **Mass-Training-Description** and **Mass-Test-Description** include many columns but we are interested in following input variables only:\n",
    "\n",
    "- `mass shape`\n",
    "- `mass margins`\n",
    "- `left or right breast`\n",
    "- `abnormality type`\n",
    "- `abnormality id`\n",
    "- `breast_density`\n",
    "- `image view`\n",
    "\n",
    "How well can we predict the **pathology type**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abd405-0d15-40c1-afdd-c177b76f19d6",
   "metadata": {},
   "source": [
    "### 3.1. (5 points) Preparing data\n",
    "\n",
    "Read the data and map to a form of $(X,Y)$\n",
    "\n",
    "i.e. matrix $X$ and a vector $Y$, where each row of $X(x_1,x_2,...,x_i)$ are input variables of a patient, and each row of $Y(y_1,y_2,...,y_i)$ is the pathology type class, for that patient. \n",
    "\n",
    "Map your categorical string to an Indicator variable. Are your classes balanced? Pandas has a neat functionalities for both these use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3840d78f-362a-4e6e-9cb8-113cea3b2dd0",
   "metadata": {},
   "source": [
    "### 3.2. (5 points) Preparing dataset\n",
    "\n",
    "Map the $(X,Y)$ to a mini-batch setting coherent with `torch.utils.data.Dataset` and `torch.utils.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ead01a-42a5-4486-aab4-e591c3472174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import trange\n",
    "import itertools    \n",
    "from sklearn.metrics import  confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b754b4d-0fd0-4015-95dc-73e928e4d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBISDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, Y):\n",
    "        # Complete the method\n",
    "        \n",
    "   # Complete the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b16a4-55ba-4aa5-a33f-fd6d759ddcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete code here\n",
    "train_dataset = CBISDataset(, )\n",
    "test_dataset = CBISDataset(, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a03cd-8e07-4a36-a70a-4e7569ef241d",
   "metadata": {},
   "source": [
    "What are the input and output dimensions of your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986c479-604f-4203-9423-98e794b5d131",
   "metadata": {},
   "source": [
    "### 3.3. Multi-layered-perceptron (5 points)\n",
    "\n",
    "Design a multi layer perceptron (MLP) with 1 hidden full connected layer. Use `ReLU` non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf130cd9-c7d4-4885-bad7-14771a5204ff",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48adcac-cadb-47b7-a40d-e9bce76612f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    # Complete the code cell\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f42f8-f7da-49aa-8c4c-db852986adac",
   "metadata": {},
   "source": [
    "### 3.4. Implement the train function (15 points) \n",
    "\n",
    "- Followed by training the network for training for 1000 epochs and batch size of 100 samples.\n",
    "- Collect average train loss over each epoch for all batch iterations. \n",
    "- Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa36b9-7994-43d0-9c38-8191e57aa6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, optimizer, criterion):\n",
    "    # Code the function\n",
    "    # Return average loss for the epoch\n",
    "    model.train()\n",
    "    losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e3516-a3c3-46e6-b5a9-ded7f89036f0",
   "metadata": {},
   "source": [
    "1. Create dataloaders to feed your network. You need to reshuffle your training samples for each epoch.\n",
    "2. Train your network using following specifications:\n",
    "    - `learning rate`: $10^{-2}$\n",
    "    - `Optimizer`: SGD \n",
    "    - `Loss Function`: CrossEntropyLoss  \n",
    "    - `Batch Size`: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4f1e9-ffd9-48e0-a26a-5afc39891248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete code here\n",
    "BATCH_SIZE = \n",
    "train_dataloader = DataLoader(, batch_size = BATCH_SIZE, )\n",
    "test_dataloader = DataLoader(, batch_size = BATCH_SIZE, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0698ddd-e562-428d-81ec-21600e2ee007",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \n",
    "optimizer = \n",
    "criterion = \n",
    "NUM_EPOCHS = \n",
    "avg_losses = []\n",
    "for epoch in trange(1, NUM_EPOCHS+1):\n",
    "    loss = train_fn(model, train_dataloader, optimizer, criterion)\n",
    "    if not epoch % 100:\n",
    "        print(f\"Average training loss for epoch {epoch}: {loss}\")\n",
    "    avg_losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b4ca7-9de6-463d-b769-3a55ec832b44",
   "metadata": {},
   "source": [
    "Plot *loss vs epoch* curve using the `losses` array:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04613267-f136-4679-b28d-538d603267c4",
   "metadata": {},
   "source": [
    "### 3.5. Implement the test function (10 points) \n",
    "\n",
    "- Evaluate your model over test data. \n",
    "- Show confusion matrix and AUROC Curve (Class wise) for both test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8aeff-0a29-4cce-abcb-27cfc0e76dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "def cm(y_true, y_pred):\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure()\n",
    "    class_names = ['0: BENIGN','1: BENIGN_WC','2: MALIGNANT']\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75eebc4-d1ec-4852-9943-d35a5a2dc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss_criterion):\n",
    "    correct = 0\n",
    "    loss = 0.0\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    # Complete the function\n",
    "    \n",
    "    \n",
    "    return loss, correct, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be86397-b4f7-49b8-bd8a-a71b683e0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_criterion = \n",
    "loss, correct, y_true, y_pred = test(model, test_dataloader ,test_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806135b-4a48-4459-a257-e9ce3a664ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call method to plot confusion matrix\n",
    "cm(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e2a23-2136-4fa1-b9dc-6f0578f21bb7",
   "metadata": {},
   "source": [
    "### 3.5. Logistic Regression (10 points)\n",
    "\n",
    "- Train a multi-class logistic regression model (in PyTorch) and compare its performance to MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d71ea6-853e-4ed1-a50e-318b96823b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticReg(nn.Module):\n",
    "    # Complete the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb9758-e5d6-40f9-af25-73528a431bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = \n",
    "optimizer = \n",
    "criterion = \n",
    "NUM_EPOCHS = \n",
    "\n",
    "avg_losses = []\n",
    "for epoch in trange(1, NUM_EPOCHS+1):\n",
    "    loss = train_fn(,,,)\n",
    "    if not epoch % 100:\n",
    "        print(f\"Average training loss for epoch {epoch}: {loss}\")\n",
    "    avg_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9af6d4-b938-48a3-b041-f5bedbfe34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_criterion = \n",
    "loss, correct, y_true, y_pred = test(,,,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d11ba4-635a-456a-8c7c-80bb00a32f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1833e00-eeda-4d7f-8ea5-7118a9191cea",
   "metadata": {},
   "source": [
    "### Bonus (10 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e404189c-f09e-4659-9810-bb39ca124a0b",
   "metadata": {},
   "source": [
    "- Compare your training and test performance. Is your model trained properly? Suggest methods to improve its performance.\n",
    "\n",
    "- For the assignment, you used SGD as the optimization function, Try [different optimizer](https://pytorch.org/docs/stable/optim.html) from PyTorch. Give sufficient reasoning if you observe any difference in performance and exlaination on its working."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
